{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=12.63s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=8.10s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faster_rcnn.utils.datasets.mscoco.dataset:need more than 0 values to unpack\n",
      "INFO:faster_rcnn.utils.datasets.mscoco.dataset:need more than 0 values to unpack\n",
      "/opt/conda/envs/pytorch/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py:24: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 1. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from faster_rcnn.utils.cython_bbox import bbox_overlaps\n",
    "from pycrayon import CrayonClient\n",
    "\n",
    "import cPickle\n",
    "from torch.optim import SGD, RMSprop, Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from datetime import datetime\n",
    "from faster_rcnn.utils.datasets.adapter import convert_data\n",
    "from faster_rcnn.utils.evaluate.metter import AverageMeter\n",
    "from faster_rcnn.utils.display.images import imshow, result_show\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "with open('sorted_index', 'rb') as fp:\n",
    "    sorted_index = cPickle.load(fp)\n",
    "\n",
    "\n",
    "# ### Đọc dữ liệu từ MS COCO dataset\n",
    "# \n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from faster_rcnn.utils.datasets.mscoco.dataset import CocoData\n",
    "from faster_rcnn.utils.datasets.data_generator import CocoGenerator\n",
    "from faster_rcnn.utils.datasets.data_generator import Enqueuer\n",
    "\n",
    "dataDir = './data/mscoco'\n",
    "dataType = 'train2014'\n",
    "annFile='%s/annotations/instances_%s.json'%(dataDir,dataType)\n",
    "batch_size = 4\n",
    "\n",
    "images_dir = os.path.join(dataDir,'images', dataType)\n",
    "cap = CocoData(root = images_dir, annFile = annFile)\n",
    "\n",
    "data_gen = CocoGenerator(data=cap, sorted_index=sorted_index, batch_size=batch_size)\n",
    "queue = Enqueuer(generator=data_gen)\n",
    "queue.start(max_queue_size=10, workers=2)\n",
    "train_data_generator = queue.get()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "dataDir = './data/mscoco'\n",
    "dataType = 'val2014'\n",
    "annFile='%s/annotations/instances_%s.json'%(dataDir,dataType)\n",
    "batch_size = 8\n",
    "\n",
    "images_dir = os.path.join(dataDir,'images', dataType)\n",
    "val_cap = CocoData(root = images_dir, annFile = annFile)\n",
    "\n",
    "val_data_gen = CocoGenerator(data=val_cap, batch_size=batch_size, shuffle=True, seed=2)\n",
    "val_queue = Enqueuer(generator=val_data_gen)\n",
    "val_queue.start(max_queue_size=10, workers=2)\n",
    "val_data_generator = val_queue.get()\n",
    "\n",
    "\n",
    "# Thử hiển thị ảnh cùng các bounding boxes\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "from faster_rcnn.faster_rcnn import FastRCNN\n",
    "\n",
    "\n",
    "# ### Tính toán feed-forward\n",
    "# \n",
    "# \n",
    "# Chúng ta sử dụng một ảnh có kích thước đầu vào là  `(width , height) = (600, 800)`\n",
    "# \n",
    "# Input:\n",
    "#     - im_data : \n",
    "#         kích thước : (batch_size, dim, witdh, height)\n",
    "#     - ground_boxes: \n",
    "#         kích thước (n, 4)\n",
    "#         \n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "categories = ['__background__'] + [x['name'] for x in cap.coco.loadCats(cap.coco.getCatIds())]\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "net = FastRCNN(categories, debug=False)\n",
    "net.cuda()\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "param = filter(lambda x: x.requires_grad, net.parameters())\n",
    "optimizer = SGD(param, lr=1e-3, momentum=0.9, weight_decay=0.0005)\n",
    "exp_lr_scheduler = StepLR(optimizer, step_size=1000, gamma=0.95)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def evaluate(data_gen ,model,tensorboard_client, steps_per_epoch=2000, current_epoch=0, current_step=0):\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = AverageMeter()\n",
    "    val_cross_entropy = AverageMeter()\n",
    "    val_loss_box = AverageMeter()\n",
    "    val_rpn_loss = AverageMeter()\n",
    "    with torch.no_grad(): \n",
    "        for step in range(1, steps_per_epoch +1):\n",
    "            blobs = data_gen.next()\n",
    "            batch_tensor, im_info, batch_boxes, batch_boxes_index = convert_data(blobs)\n",
    "            model(batch_tensor, im_info, batch_boxes, batch_boxes_index)\n",
    "            loss = model.loss\n",
    "            val_loss_box.update(model.loss_box.item())\n",
    "            val_cross_entropy.update(model.cross_entropy.item())\n",
    "            val_loss.update(loss.item())\n",
    "            val_rpn_loss.update(model.rpn.loss.item())\n",
    "            del loss\n",
    "            del batch_tensor\n",
    "            del blobs\n",
    "            \n",
    "            if step % 30 == 1:\n",
    "                log_text = 'epoch: %d : step %d,  val_loss: %.4f at %s' % (\n",
    "                    current_epoch + 1, step , train_loss.avg, datetime.now().strftime('%m\\%d_%H:%M'))\n",
    "                print(log_text)\n",
    "\n",
    "    log_text = 'val_loss: %.4f at epoch %d' % (val_loss.avg, current_epoch + 1)\n",
    "    print(log_text)\n",
    "    tensorboard_client.add_scalar_value('val_loss', val_loss.avg, step=current_step)\n",
    "    tensorboard_client.add_scalar_value('val_rpn_loss', val_rpn_loss.avg, step=current_step)\n",
    "    tensorboard_client.add_scalar_value('val_cross_entropy', val_cross_entropy.avg, step=current_step)\n",
    "    tensorboard_client.add_scalar_value('val_loss_box', val_loss_box.avg, step=current_step)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "def train(data_gen ,model, tensorboard_client, metters, optimizer, lr_scheduler,steps_per_epoch=2000, current_epoch=0, current_step=0):\n",
    "    model.train()\n",
    "    train_loss , cross_entropy , loss_box, rpn_loss = metters\n",
    "    for step in range(1, steps_per_epoch +1):\n",
    "        lr_scheduler.step()        \n",
    "        blobs = data_gen.next()\n",
    "        batch_tensor, im_info, batch_boxes, batch_boxes_index = convert_data(blobs)\n",
    "        optimizer.zero_grad()\n",
    "        model(batch_tensor, im_info, batch_boxes, batch_boxes_index)\n",
    "        cross_entropy.update(model.cross_entropy.item())\n",
    "        loss_box.update(model.loss_box.item())\n",
    "        train_loss.update(model.loss.item())\n",
    "        rpn_loss.update(model.rpn.loss.item())\n",
    "        loss = model.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_step = current_epoch * steps_per_epoch + step\n",
    "        if step % 30 == 1:\n",
    "            log_text = 'epoch: %d : step %d,  loss: %.4f at %s' % (\n",
    "                current_epoch + 1, step , train_loss.avg, datetime.now().strftime('%m/%d_%H:%M'))\n",
    "            print(log_text)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            tensorboard_client.add_scalar_value('train_loss', train_loss.avg, step=current_step)\n",
    "            tensorboard_client.add_scalar_value('rpn_loss', rpn_loss.avg, step=current_step)\n",
    "            tensorboard_client.add_scalar_value('cross_entropy', cross_entropy.avg, step=current_step)\n",
    "            tensorboard_client.add_scalar_value('loss_box', loss_box.avg, step=current_step)\n",
    "    \n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "def training(train_data_gen, val_data_gen, optimizer, lr_scheduler ,model, epochs=10):\n",
    "    \n",
    "    exp_name = datetime.now().strftime('vgg16_%m-%d_%H-%Ms')\n",
    "    cc = CrayonClient(hostname=\"crayon\", port=8889)\n",
    "    exp = cc.create_experiment(exp_name)\n",
    "    \n",
    "    \n",
    "    train_loss = AverageMeter()\n",
    "    cross_entropy = AverageMeter()\n",
    "    loss_box = AverageMeter()\n",
    "    rpn_loss = AverageMeter()\n",
    "    metters = (train_loss , cross_entropy , loss_box, rpn_loss)\n",
    "    steps_config = {\n",
    "        'train_step_per_epoch' : 20000,\n",
    "        'val_step_per_epoch' : 5000\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #train(data_gen ,model, exp, metters, optimizer, lr_scheduler, steps_per_epoch=steps_config['train_step_per_epoch'], current_epoch=epoch)\n",
    "        current_step = steps_config['train_step_per_epoch'] * (epoch + 1)\n",
    "        #torch.save(model.state_dict(), './checkpoints/faster_model_at_epoch_%d.pkl' % (epoch + 1)) \n",
    "\n",
    "        evaluate(data_gen ,model,exp, steps_per_epoch=steps_config['val_step_per_epoch'], current_epoch=epoch, current_step=current_step)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "faster_rcnn/faster_rcnn.py:277: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-71452bac35dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_generator\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-eb2f835d635b>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(train_data_gen, val_data_gen, optimizer, lr_scheduler, model, epochs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m#torch.save(model.state_dict(), './checkpoints/faster_model_at_epoch_%d.pkl' % (epoch + 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_step_per_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-eb2f835d635b>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(data_gen, model, tensorboard_client, steps_per_epoch, current_epoch, current_step)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mbatch_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_boxes_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_boxes_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0mval_loss_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mval_cross_entropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/faster_rcnn/faster_rcnn.pyc\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mDescription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \"\"\"\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_box\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "training(train_data_generator, val_data_generator ,optimizer=optimizer,lr_scheduler=exp_lr_scheduler, model=net, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
