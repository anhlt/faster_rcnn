{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(100000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 100 seconds\n"
     ]
    }
   ],
   "source": [
    " %autosave 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian concept learning\n",
    "\n",
    "We can think of learning the meaning of a word as concept learning , which in turn is quivanlen to binnary classfication\n",
    "\n",
    "To see this, define $f(x) = 1$ if $x$ is an example of the concept $\\mathcal{C}$ and $f(x) = 0$ otherwise.The the goal is to learn the indicator function $f$ , which just defines which elements are in the set $\\mathcal{C}$. By allowing for uncertainty about the definition of $f$, or equivanlently the elements of C , we can emulate __fuzzy set theory__, but using standard probability calculus. \n",
    "\n",
    "Suppose, for simplicity, that all numbers are intergers between 1 and 100. Now suppose I tell you \"16\" is a positive example of the concept. What the other do you think are the positive? 17, 6, 32, 99? It's hard to tell with only one example, so your prediction will be quite vague. Presumably numbers that are similar in some sense to 16 are more likely. But what similar in waht way? 17 is similar, because it is \"close by\", 6 is similar because it has a digit in common, 32 is similar because it is also even and power of 2, but 99 does not seem similar. Thus sime number are more likely than others. We can present this as a probability distribution, $p(\\tilde{x} \\mid \\mathcal{D})$ , which is the probability $\\tilde{x} \\in C$ given the data $\\mathcal{D}$ for any $\\tilde{x} \\in \\{ 1 , \\cdots , 100 \\}$. This is called the __posterior predictive distribution__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now suppose I tell you that 8,2 and 64 also positive examples. Now you may guess that the hidden concept is \"powers of two\". This is an example of __induction__. Given this hypothesis, the predictive distribution is quite specific, and puts most of its mass on powers of 2, as shown in Figure 3.1. If instead I tell you the data is $D = \\{16,23,19,20\\}$, you will get a diffirent kind of __generalization gradient__, as shown in Figure 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How can we explain this behavior and emulate it in a machine? The classic approach to induictin ts to suppose we have a __hypothsis space__ of concepts, $\\mathcal{H}$ , such as: add numbers, even numbers, all numbers between 1 and 100, powers of two, all numbers ending in $j$ (for $ 0 \\leq j \\leq 9$). The subset of H is consistent with the data $D$ is called the __version space__. As we see more examples, the version space shinks and we become increasingly certain about the concept.\n",
    "\n",
    "However, the version space is not the whole story. After seeing $\\mathcal{D} = \\{16\\}$, there are many consistent rules; how do you combine them to predict if $\\tilde{x} \\in \\mathcal{C}$ ? Also, after seeing $\\mathcal{D} = \\{16,8,2,64\\}$, why did you choose the rule \"power of 2\" and not say, \"all even numbers\", or \"powers of 2 except for 32\", both of wich are equally consistent with the evidence. We will now provide a Bayesian explaination for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood\n",
    "\n",
    "We must explain why we chose $h_{two} \\triangleq$ \"powers of two\", and not say $h_even \\triangleq$ \"even numbers\" after seeing $\\mathcal{D} = \\{16, 8 , 2 , 64\\}$, given that both hypotheses are consistent with the evidence. The key intuition is that we want to avoid __suspicious coincidence__. If the true concept was even number, how come we only saw numbers that happened to be powers of two?\n",
    "\n",
    "To formalize this, let us assume that examples are sample uniformly at random from the __extension__ of a concept. The extrension of a concept is just the set of numbers that belong to it, e.g, the extension of $h_{even}$ is $\\{2,4,6, \\cdots, 98, 100\\}$. the extension of \"numbers ending in 9\" is $\\{9,19,29, \\cdots, 99\\}$.\n",
    "Given that assumption, the probability of independently sampling $\\mathcal{N}$ items from $h$ is given by:\n",
    "$$ p(\\mathcal{D} \\mid h) = \\big[ \\frac{1}{size(h)} \\big]^N = \\big[ \\frac{1}{\\mid h \\mid} \\big]^N$$\n",
    "\n",
    "\n",
    "Let $\\mathcal{D} = \\{16\\}$. Then $p(\\mathcal{D} \\mid h_{two}) = 1/6$, since there are only 6 powers of two less than 100, but $p(\\mathcal{D} \\mid h_{even}) = 1/50$, since there are 50 even numbers. So the likeihood that $h = h_{tow}$ is higher than if $h = h_{even}$ . After 4 examples, the likelihood of $h_{two}$ is $(1/6)^4$, whereas the likelihood of $h_{even}$ is $(1/50)^4$. This is a __likelihood ratio__ of almost 5000:1 in favor of $h_{two}$. The quantifies our earlier intuition that $\\mathcal{D}= \\{16,8,2,64\\}$ would be very suspicious coincidence if generated by $h_{even}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior\n",
    "Suppose $\\mathcal{D} = \\{16,8,2,64 \\}$. Given this data, the concept $h^{'}$ = \"powers of two except 32\" is more likely than $h$ = \"powers of two\", since $h^{'}$ does not need to explain the coincidence that 32 is missing from the set of examples.\n",
    "\n",
    "However, the hypothesis $h^{'}$ seems \"conceptually unnatural\". We can capture suc intuition by assigning low prior probalility to unnatural concepts. Of course, your prior might be diffirent than mine. This __subjective__ aspect of Bayesian reasoning is a source of much controversy, since it means , for example, that a child and a math professor will reach diffirent answers. In fact, they presumably not only have diffirent priors, but alos diffirent hypothesis spaces. However, we can finnesse that by defining the hypothesis space of the child and the math professor to be the same, and then setting the child\"s prior weight to be zero on certain \"advanced\" concepts. Thus there is no sharp distinction between the prior and the hypothesis space.\n",
    "\n",
    "Although the subjectivity of the prior is controvesial, it is acctually quite useful. If you are told the number from some arithmetic rule, then given 1200, 1500, 900 and 1400, you may think 400 is likely but 1183 is unlikely. But you are told that the numbers are examples of healthy cholesterol levels, you woukd probably think 400 is unlikely and 1183 is likely. Thus we see the the prior is mechanism by which background knownledge can be brought to bear on a problem. Without this, rapid learning is immposible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior\n",
    "\n",
    "The posterior is simply the likelihood times the prior, normalized. In this context we have\n",
    "$$ p(h \\mid \\mathcal{D}) = \\frac{ p(\\mathcal{D} \\mid h) p(h) } { \\sum_{h^{'} \\in \\mathcal{D}} p(\\mathcal{D}, h^{'})  } = \\frac{ p(h) \\prod (\\mathcal{D} \\in h) / {\\mid h \\mid}^N } { \\sum_{h^{'} \\in \\mathcal{D}} p(h^{'}) \\prod (\\mathcal{D} \\in h^{'}) / {\\mid h^{'} \\mid}^N } $$\n",
    "\n",
    "where $\\prod (\\mathcal{D} \\in h)$ is 1 __iff__ all the data are in the extension of the hypothesis h. \n",
    "\n",
    "We see that the posterior is combination or the prior and likelihood. In the case of most of the concepts, the prior is uniform, so the postterior is proportional to the likelihood. However, the \"unnatural\" concepts have low posterior support, dispite having high likelihood, due to the low prior. Conversely, the concept of odd numbers has low posterior, dispite having a high prior, due to the low likelihood.\n",
    "\n",
    "In general, when we have enough data, the posterior $p(h \\mid \\mathcal{D})$ becomes peaked on a single concept, namely the MAP estimate, i.e,\n",
    "$$ p (f \\mid \\mathcal{D}) \\rightarrow \\delta_{\\hat{h}^{MAP}} (h)$$\n",
    "\n",
    "where $\\hat{h}^{MAP} = argmax_h p(h \\mid \\mathcal{D})$ is posterior mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
