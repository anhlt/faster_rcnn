{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "from scipy.misc import imread\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import torch\n",
    "from torch.autograd import  Variable\n",
    "from faster_rcnn.utils.images import imshow\n",
    "from faster_rcnn.fastrcnn.bbox_transform import bbox_transform\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sorted_index', 'rb') as fp:\n",
    "    sorted_index = cPickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đọc dữ liệu từ MS COCO dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.17s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from faster_rcnn.utils.datasets.mscoco.dataset import CocoData\n",
    "from faster_rcnn.utils.datasets.data_generator import CocoGenerator\n",
    "from faster_rcnn.utils.datasets.data_generator import Enqueuer\n",
    "\n",
    "dataDir = './data/mscoco'\n",
    "dataType = 'train2014'\n",
    "annFile='%s/annotations/instances_%s.json'%(dataDir,dataType)\n",
    "pre_proposal_folder = './data/mscoco/coco_proposals/MCG/'\n",
    "\n",
    "images_dir = os.path.join(dataDir,'images', dataType)\n",
    "cap = CocoData(root = images_dir, annFile = annFile)\n",
    "\n",
    "data_gen = CocoGenerator(data=cap, sorted_index=sorted_index, batch_size=3)\n",
    "queue = Enqueuer(generator=data_gen)\n",
    "queue.start(max_queue_size=10, workers=2)\n",
    "t = queue.get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPN\n",
    "\n",
    "#### Mục đích \n",
    "\n",
    "- Do Fast RCNN vẫn phụ thuộc vào Selective search để tính toán các proposal region dẫn đến kết quả tính toán rất chậm\n",
    "- RPN dùng để tính toán proposal region mà không cần phụ thuộc vào các thuật toán khác\n",
    "\n",
    "#### Cách hoạt động\n",
    "\n",
    "- Sử dụng một cửa sổ trượt trên feature map\n",
    "- tạo một network nhỏ để tính\n",
    "    - Phân loạt một anchor có chứa hay không chưa object (Anchor good / bad)\n",
    "    - Tính toán các trị số của proposal regions (box regression)\n",
    "    \n",
    "- Vị trí của cửa sổ trượt trên feature map, cho biết thông tin về vị trí của anchor trên ảnh gốc\n",
    "\n",
    "#### Input\n",
    "\n",
    "- Hình ảnh: là tensor có kích thước\n",
    "\n",
    "    $[batchsize, dim, im\\_height, im\\_width]$\n",
    "    \n",
    "- bounding box:\n",
    "    \n",
    "    $(x_1, y_1, x_2, y_2)$\n",
    "\n",
    "    - $x_1, y_1$ : tọa độ x,y của điểm trái dưới (lower-left)\n",
    "    - $x_2, y_2$ : tọa độ x,y của điểm phải trên (top - right)\n",
    "\n",
    "#### Output\n",
    "\n",
    "- RPN classification (anchor good / bad)\n",
    "- RPN regression (anchor -> proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cùng xem định dạng của một input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = t.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_height = np.max([blob['tensor'].shape[2] for blob in blobs])\n",
    "max_width = np.max([blob['tensor'].shape[3] for blob in blobs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tensor = torch.Tensor(3, 3, max_height, max_width).fill_(0).type_as(blob['tensor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, blob in enumerate(blobs):\n",
    "    shape = blob['tensor'].shape\n",
    "    batch_tensor[i,:, :shape[2], :shape[3] ]= blob['tensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[[ 453.125       793.75        590.625       868.75      ]\n",
      " [   0.          315.625       496.875       959.375     ]\n",
      " [ 171.875       159.375       587.5         343.75      ]\n",
      " [ 400.         1421.875       596.875      1612.5       ]\n",
      " [ 312.5         634.375       515.625       709.375     ]\n",
      " [   0.          934.375        18.75       1018.75      ]\n",
      " [ 220.10050251  337.68844221  301.50753769  780.90452261]\n",
      " [  24.12060302    0.          569.84924623 1878.3919598 ]\n",
      " [  18.29268293  706.09756098  303.65853659  892.68292683]\n",
      " [ 351.2195122  1628.04878049  482.92682927 1690.24390244]\n",
      " [  10.97560976  918.29268293  585.36585366 1342.68292683]\n",
      " [   0.         1364.63414634  585.36585366 1803.65853659]\n",
      " [  18.29268293   14.63414634  563.41463415  435.36585366]\n",
      " [ 365.85365854  951.2195122   380.48780488 1031.70731707]\n",
      " [ 413.41463415 1408.53658537  435.36585366 1481.70731707]\n",
      " [ 384.14634146  951.2195122   391.46341463 1031.70731707]\n",
      " [ 402.43902439 1412.19512195  413.41463415 1481.70731707]\n",
      " [ 384.14634146  490.24390244  446.34146341  574.3902439 ]\n",
      " [ 420.73170732  958.53658537  435.36585366 1028.04878049]\n",
      " [ 391.46341463  951.2195122   402.43902439 1031.70731707]\n",
      " [ 435.36585366  958.53658537  442.68292683 1031.70731707]\n",
      " [  10.97560976  464.63414634  581.70731707  896.34146341]\n",
      " [ 325.6097561  1093.90243902  453.65853659 1243.90243902]\n",
      " [  95.12195122  150.          267.07317073  278.04878049]\n",
      " [ 318.29268293  958.53658537  365.85365854 1028.04878049]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faster_rcnn.utils.datasets.mscoco.dataset:need more than 0 values to unpack\n"
     ]
    }
   ],
   "source": [
    "total_boxes = 0\n",
    "batch_boxes = np.empty((0, 4))\n",
    "batch_boxes_index = np.empty((0,), dtype=np.int)\n",
    "\n",
    "for i, blob in enumerate(blobs):\n",
    "    total_boxes = blob['boxes'].shape[0]\n",
    "    batch_boxes = np.vstack((batch_boxes, blob['boxes']))\n",
    "    a =  np.zeros((total_boxes , ), dtype=np.int)\n",
    "    a.fill(i)\n",
    "    batch_boxes_index = np.concatenate((batch_boxes_index, a), axis=0)\n",
    "    \n",
    "print batch_boxes_index\n",
    "print batch_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thử hiển thị ảnh cùng các bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs[0]['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(batch_tensor[0], blobs[0]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(batch_tensor[1], blobs[1]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(batch_tensor[2], blobs[2]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from faster_rcnn.faster_rcnn import  RPN\n",
    "rpn_network = RPN()\n",
    "rpn_network.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tính toán feed-forward\n",
    "\n",
    "\n",
    "Chúng ta sử dụng một ảnh có kích thước đầu vào là  `(width , height) = (600, 800)`\n",
    "\n",
    "Input:\n",
    "    - im_data : \n",
    "        kích thước : (batch_size, dim, witdh, height)\n",
    "    - ground_boxes: \n",
    "        kích thước (n, 4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tính `cnn features`  \n",
    "\n",
    "Sử dụng mạng CNN đã được train trên Imagenet, cụ thể là VGG16 để tính toán `cnn features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tensor to variable\n",
    "x = Variable(batch_tensor).cuda()\n",
    "cnn_feature = rpn_network.features(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta nhận được cnn_features có kích thước là  (37 , 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print cnn_feature.shape\n",
    "print x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_height, feature_width = cnn_feature.shape[2], cnn_feature.shape[3]\n",
    "im_height, im_width = x.shape[2] , x.shape[3]\n",
    "batch_size = cnn_feature.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nhận xét\n",
    "\n",
    "Sau khi feed-forward một lần đối với ảnh gốc, thu được convolutional features của ảnh đó. Ví dụ với một hình ảnh có kích thước $600 * 800 * 3$, ta sẽ thu được convolutional features với kích thước $37 * 50 * 512$. Kích thước của features bị giảm nhỏ khoảng 16 lần $\\frac{600}{37}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnchorTargerLayer\n",
    "\n",
    "Sau khi thu được Convolutional Feature, chúng ta sẽ cùng tìm hiểu về AnchorTargetLayer.\n",
    "\n",
    "### Vai trò của AnchorTargetlayer\n",
    "    Gán cho anchor tương ứng với ground-truth boxes. \n",
    "    Tạo ra nhãn tương ứng cho từng anchor (bằng cách tính toán IOU)\n",
    "    Tính giá trị mục tiêu của bounding-box regression\n",
    "\n",
    "Tính toán đầu ra của RPN\n",
    "\n",
    "- rpn_labels : (HxWxA, 1), for each anchor, 0 denotes bg, 1 fg, -1 dontcare\n",
    "- rpn_bbox_targets: (HxWxA, 4), distances of the anchors to the gt_boxes(may contains some transform) that are the regression objectives\n",
    "- rpn_bbox_inside_weights: (HxWxA, 4) weights of each boxes, mainly accepts hyper param in cfg\n",
    "- rpn_bbox_outside_weights: (HxWxA, 4) used to balance the fg/bg, beacuse the numbers of bgs and fgs mays significiantly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_rcnn.rpn_msr.anchor_target_layer_2 import AnchorTargerLayer\n",
    "anchor_target_layer = AnchorTargerLayer(feat_stride=[16,], anchor_scales=[8, 16 ,32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tạo các anchor tương ứng với vị trí ảnh tại vị trí [0. , 0. , 0., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, rpn_bbox_pred, rpn_cls_score =  rpn_network._computer_forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_info = torch.Tensor([[im_height, im_width]])\n",
    "batch_boxes_index = torch.Tensor(batch_boxes_index)\n",
    "batch_boxes = torch.Tensor(batch_boxes).numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anchors = anchor_target_layer._create_anchors(feature_height, feature_width)\n",
    "total_anchors = all_anchors.shape[0]\n",
    "\n",
    "# only keep anchors inside the image\n",
    "inside_anchors, inside_anchor_indexes = anchor_target_layer._filter_outside_anchors(\n",
    "    all_anchors, im_height, im_width)\n",
    "print all_anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = anchor_target_layer.calculate_target(inside_anchors, batch_size, inside_anchor_indexes, batch_boxes, batch_boxes_index.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = anchor_target_layer((rpn_cls_score, torch.Tensor(batch_boxes), im_info, batch_boxes_index))\n",
    "print(result[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print target[0].shape\n",
    "print target[1].shape\n",
    "print result[0][1].numpy() == 1\n",
    "print all_anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "imshow(batch_tensor[i], blobs[i]['boxes'] ,inside_anchors[target[0][i] == 1] )\n",
    "# imshow(batch_tensor[i], blobs[i]['boxes'] ,all_anchors[result[0][i].numpy() == 0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
